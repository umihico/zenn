---
title: ""
emoji: "👌"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

データに対して、LLMを走らせたいときありませんか。それも多くのレコードに対して走らせたいし、かつLLMの処理結果を眺めてプロンプトを磨いて再実行したいときありませんか。

PoC的にそんなことをやりたい状況があったので、手元のMacbook Airでも動くローカルLLMを使う方法を模索しました。

今回は、TinySwallow, LM Studio、出力をチャット形式でなくJSON化、CLIだけで環境構築する、という組み合わせがなかったので、記事にしました

Sakana AIが開発したTinySwallowを使うことでPCスペックを問わず使えることができます。

ハイライト

- ５分で、CLIを打つだけで、無課金でローカルLLMを導入できる
- Sakana AIが開発した日本語特化のTinySwallowを使うことでPCスペックを問わず使える
- マルチプロセスで動かすことができる
- OpenAIのAPIフォーマットでJSON強制することでプログラム処理フレンドリーな結果を得られる
