---
title: "MacでローカルLLM環境を5分で構築：TinySwallow × LM Studio × JSON構造化出力"
emoji: "🐦"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ai", "llm", "lmstudio", "json", "macos"]
published: false
---
<!-- emoji候補
2. 🤖 (AI/ロボット)
3. 💻 (コンピュータ)
4. 🚀 (高速/クイックスタート)
5. 🔧 (ツール/構築)
6. 🏠 (ローカル環境)
7. 📦 (パッケージ/インストール)
8. ⚡ (高速/効率)
9. 🎯 (的確/精度)
10. 🧠 (知能/思考)
-->

## きっかけ：大量データにLLMを適用したい

データ分析やPoC開発で、数百〜数千件のレコードに対してLLM処理を実行したいケースがありました。さらに、処理結果を眺めながらプロンプトを改善し、再実行を繰り返したい。そんな要求を満たすローカルLLM環境を、MacBook Air（M2、メモリ8GB）で構築した手順を共有します。

## この記事で実現できること

- **5分でCLIのみで環境構築完了**
- **無課金でローカルLLM環境を実現**
- **低スペックPCでも動作（メモリ8GB〜）**
- **JSON構造化出力でプログラム連携が容易**
- **OpenAI互換APIでコード資産を流用可能**

## 対象読者と前提知識

- ローカルでLLMを動かしたい開発者
- APIコストを抑えたい方
- 大量データ処理でLLMを活用したい方
- 前提：macOS（Apple Silicon）、基本的なCLI操作

## TL;DR（結論先出し）

```bash
# 1. インストール
brew install --cask lm-studio

# 2. GUI起動（初回のみ必須）
open -a "LM Studio"

# 3. CLI有効化
~/.lmstudio/bin/lms bootstrap

# 4. サーバー起動
lms server start --port 1234

# 5. モデルダウンロード（対話的に選択）
lms get --gguf "TinySwallow-1.5B-Instruct"

# 6. モデルロード
lms load tinyswallow-1.5b-instruct --identifier tinyswallow --gpu max --context-length 4096

# 7. JSON構造化出力テスト
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyswallow",
    "messages": [{"role":"user","content":"タスクを3つ生成"}],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "Tasks",
        "strict": true,
        "schema": {
          "type": "object",
          "properties": {
            "tasks": {
              "type": "array",
              "items": {"type": "object"}
            }
          }
        }
      }
    }
  }'
```

## なぜTinySwallow × LM Studioなのか

### TinySwallowを選んだ理由

1. **軽量（1.5Bパラメータ）**: 8GBメモリのMacBook Airでも快適動作
2. **日本語特化**: Sakana AIが開発した日本語に最適化されたモデル
3. **高速推論**: 小規模ながら実用的な精度を維持
4. **GGUF形式対応**: 量子化により更なる軽量化が可能

### LM Studioを選んだ理由

1. **CLI完結**: GUIを起動せずにヘッドレス運用可能
2. **OpenAI互換API**: 既存のコードをそのまま流用
3. **JSON Schema対応**: 構造化出力で確実なデータ取得
4. **Homebrew対応**: インストールが簡単

## 動作環境

- **OS**: macOS 13.4以上（MLXは14.0以上推奨）
- **CPU**: Apple Silicon（M1/M2/M3）必須
- **メモリ**: 8GB以上（16GB推奨）
- **ストレージ**: 5GB以上の空き容量
- **LM Studio**: v0.3.23以上
- **モデル**: TinySwallow-1.5B-Instruct (Q8_0, 1.65GB)

## セットアップ手順

### 1. LM Studioのインストール

Homebrewを使用してインストールします：

```bash
brew install --cask lm-studio
```

実行結果：
```
==> Downloading https://installers.lmstudio.ai/darwin/arm64/0.3.23-3/LM-Studio-0.3.23-3-arm64.dmg
==> Installing Cask lm-studio
==> Moving App 'LM Studio.app' to '/Applications/LM Studio.app'
🍺  lm-studio was successfully installed!
```

### 2. 初回起動とCLI有効化

LM Studioは初回だけGUIを起動する必要があります：

```bash
# GUI起動（初回のみ必須）
open -a "LM Studio"

# CLIツールをPATHに追加
~/.lmstudio/bin/lms bootstrap
```

bootstrap実行結果：
```
┌ LM Studio CLI Installation ─────────────────────────────────────────────────┐
│                                                                              │
│   ✓ Already Installed                                                       │
│                                                                              │
│   LM Studio CLI tool is already installed for the following shells:         │
│                                                                              │
│   · zsh (~/.zshrc)                                                         │
│                                                                              │
│   If your shell is not listed above, please try to add the following        │
│   directory to the PATH environment variable:                              │
│                                                                              │
│   /Users/username/.lmstudio/bin                                            │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

新しいターミナルを開いて確認：

```bash
source ~/.zshrc && lms version
```

出力：
```
   __   __  ___  ______          ___        _______   ____
  / /  /  |/  / / __/ /___ _____/ (_)__    / ___/ /  /  _/
 / /__/ /|_/ / _\ \/ __/ // / _  / / _ \  / /__/ /___/ /  
/____/_/  /_/ /___/\__/\_,_/\_,_/_/\___/  \___/____/___/  

lms - LM Studio CLI - v0.0.46
GitHub: https://github.com/lmstudio-ai/lms
```

### 3. サーバー起動

OpenAI互換APIサーバーを起動します：

```bash
lms server start --port 1234
```

出力：
```
Success! Server is now running on port 1234
```

### 4. TinySwallowモデルのダウンロード

対話的にモデルを選択してダウンロードします：

```bash
lms get --gguf "TinySwallow-1.5B-Instruct"
```

選択画面が表示されるので：
1. `SakanaAI/TinySwallow-1.5B-Instruct-GGUF`を選択
2. `Q8_0`（推奨）を選択

```
Searching for models with the term TinySwallow-1.5B-Instruct
No exact match found. Please choose a model from the list below.

? Select a model to download 
  ❯ SakanaAI/TinySwallow-1.5B-Instruct-GGUF
    bartowski/TinySwallow-1.5B-Instruct-GGUF
    koguma-ai/SakanaAI-TinySwallow-1.5B-Instruct-GRPO.gguf

? Select an option to download Tinyswallow 1.5B Instruct
  ❯ [Q8_0] (1.65 GB)
    [Q4_K_M] (1.01 GB)
    [Q5_K_M] (1.17 GB)

Downloading Tinyswallow 1.5B Instruct [Q8_0] (1.65 GB)
Finalizing download...
Download completed. You can load the model with: 
    lms load tinyswallow-1.5b-instruct
```

### 5. モデルのロード

ダウンロードしたモデルをメモリにロードします：

```bash
lms load tinyswallow-1.5b-instruct \
  --identifier tinyswallow \
  --gpu max \
  --context-length 4096
```

プログレスバーが表示され、完了すると：
```
✓ Model loaded successfully
```

### 6. 動作確認

モデル一覧を確認：

```bash
curl http://localhost:1234/v1/models
```

レスポンス：
```json
{
  "data": [
    {
      "id": "tinyswallow",
      "object": "model",
      "owned_by": "organization_owner"
    }
  ],
  "object": "list"
}
```

## JSON構造化出力の実装

### 基本的な使い方

OpenAI互換APIの`response_format.json_schema`を使用して、確実にJSON形式で応答を取得できます：

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyswallow",
    "messages": [{"role":"user","content":"3件のタスクをJSONで"}],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "Tasks",
        "strict": true,
        "schema": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "tasks": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "title": {"type": "string"},
                  "priority": {"type": "string", "enum": ["low", "med", "high"]}
                },
                "required": ["title", "priority"],
                "additionalProperties": false
              },
              "minItems": 3,
              "maxItems": 3
            }
          },
          "required": ["tasks"]
        }
      }
    },
    "stream": false
  }'
```

### レスポンス例

```json
{
  "choices": [
    {
      "message": {
        "content": "{\"tasks\":[{\"title\":\"顧客情報管理\",\"priority\":\"high\"},{\"title\":\"マーケティングキャンペーン分析\",\"priority\":\"low\"},{\"title\":\"製品開発アイデアの生成\",\"priority\":\"med\"}]}"
      }
    }
  ]
}
```

JSONを抽出して整形：

```bash
# jqを使用してJSONを抽出・整形
curl -s http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ ... }' | jq -r '.choices[0].message.content' | jq .
```

結果：
```json
{
  "tasks": [
    {
      "title": "顧客情報管理",
      "priority": "high"
    },
    {
      "title": "マーケティングキャンペーン分析",
      "priority": "low"
    },
    {
      "title": "製品開発アイデアの生成",
      "priority": "med"
    }
  ]
}
```

## 実践例：包括的な自己評価システム

TinySwallowの能力を最大限に引き出す例として、21項目×2フィールド（スコア＋理由）= 42フィールドの複雑なJSON構造化を実装しました：

```bash
curl -s http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyswallow",
    "messages": [{"role":"user","content":"あなた自身を包括的に自己評価してください"}],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "SelfEvaluation",
        "strict": true,
        "schema": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "friendliness": {"type": "integer", "minimum": 0, "maximum": 10},
            "friendliness_reason": {"type": "string"},
            "reliability": {"type": "integer", "minimum": 0, "maximum": 10},
            "reliability_reason": {"type": "string"},
            "creativity": {"type": "integer", "minimum": 0, "maximum": 10},
            "creativity_reason": {"type": "string"},
            "intelligence": {"type": "integer", "minimum": 0, "maximum": 10},
            "intelligence_reason": {"type": "string"},
            "accuracy": {"type": "integer", "minimum": 0, "maximum": 10},
            "accuracy_reason": {"type": "string"}
          },
          "required": ["friendliness", "friendliness_reason", "reliability", "reliability_reason", "creativity", "creativity_reason", "intelligence", "intelligence_reason", "accuracy", "accuracy_reason"]
        }
      }
    }
  }' | jq -r '.choices[0].message.content' | jq .
```

レスポンス例（抜粋）：
```json
{
  "friendliness": 8,
  "friendliness_reason": "ユーザーとの交流が好きであり、役立つ情報を提供することを目指しています",
  "reliability": 9,
  "reliability_reason": "正確で信頼性の高い情報源であることを目指しており、約束を守ることに強い意志を持っています",
  "creativity": 7,
  "creativity_reason": "創造的なテクニックを使用してユーザーに問題解決や新しいアイデアを提供できます",
  "intelligence": 10,
  "intelligence_reason": "幅広いトピックについて知識を持っており、正確な答えを提供することを目指しています",
  "accuracy": 10,
  "accuracy_reason": "正確で信頼できる情報源であることを目指しており、誤りを最小限に抑えることができます"
}
```

## Pythonでの実装例

OpenAI互換APIなので、既存のコードをそのまま流用できます：

```python
from openai import OpenAI
import json

# ローカルのLM Studioに接続
client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"  # ローカルなので不要
)

# タスクリストのスキーマ定義
task_schema = {
    "name": "TaskList",
    "strict": True,
    "schema": {
        "type": "object",
        "properties": {
            "tasks": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "integer"},
                        "title": {"type": "string"},
                        "description": {"type": "string"},
                        "priority": {"type": "string", "enum": ["low", "medium", "high"]},
                        "estimated_hours": {"type": "number"}
                    },
                    "required": ["id", "title", "priority"]
                }
            },
            "total_hours": {"type": "number"}
        },
        "required": ["tasks", "total_hours"]
    }
}

# APIリクエスト
response = client.chat.completions.create(
    model="tinyswallow",
    messages=[
        {"role": "user", "content": "今週のタスクを5つ生成してください"}
    ],
    response_format={
        "type": "json_schema",
        "json_schema": task_schema
    }
)

# レスポンスをパース
result = json.loads(response.choices[0].message.content)
print(json.dumps(result, ensure_ascii=False, indent=2))

# データフレームに変換して分析
import pandas as pd
df = pd.DataFrame(result["tasks"])
print(f"\n高優先度タスク数: {len(df[df['priority'] == 'high'])}")
print(f"総見積時間: {result['total_hours']}時間")
```

## 大量データ処理への応用

CSVファイルの各行に対してLLM処理を実行する例：

```python
import pandas as pd
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json
from openai import OpenAI

# クライアント初期化
client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")

def process_record(row):
    """各レコードをLLMで処理"""
    response = client.chat.completions.create(
        model="tinyswallow",
        messages=[
            {"role": "user", "content": f"以下のデータを分析してください: {row.to_dict()}"}
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "Analysis",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "category": {"type": "string"},
                        "sentiment": {"type": "string", "enum": ["positive", "neutral", "negative"]},
                        "score": {"type": "number", "minimum": 0, "maximum": 100},
                        "summary": {"type": "string"}
                    },
                    "required": ["category", "sentiment", "score", "summary"]
                }
            }
        }
    )
    
    result = json.loads(response.choices[0].message.content)
    return {**row.to_dict(), **result}

# CSVデータ読み込み
df = pd.read_csv("data.csv")

# マルチスレッドで並列処理
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(process_record, [row for _, row in df.iterrows()]))

# 結果をデータフレームに変換
result_df = pd.DataFrame(results)
result_df.to_csv("analyzed_data.csv", index=False)

print(f"処理完了: {len(result_df)}件")
print(f"ポジティブ: {len(result_df[result_df['sentiment'] == 'positive'])}件")
print(f"平均スコア: {result_df['score'].mean():.2f}")
```

## パフォーマンスと制限事項

### 実測パフォーマンス（MacBook Air M2 8GB）

- **モデルロード時間**: 約3秒
- **初回レスポンス**: 約1.5秒
- **平均応答速度**: 20-30トークン/秒
- **メモリ使用量**: 2-3GB（モデルロード時）
- **並列処理**: 4プロセスまで安定動作

### 制限事項と回避策

1. **7B未満モデルの制約**
   - 複雑なスキーマでは不安定になる場合がある
   - 回避策：スキーマを簡略化、リトライロジックの実装

2. **コンテキスト長の制限**
   - 最大4096トークン
   - 回避策：長文は分割して処理

3. **JSON生成の失敗**
   - まれに不正なJSONを生成
   - 回避策：try-catchでエラーハンドリング、再試行

## ヘッドレス運用（常駐化）

システム起動時に自動的にLM Studioサーバーを起動する設定：

### launchdを使用した自動起動

`~/Library/LaunchAgents/com.lmstudio.server.plist`を作成：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.lmstudio.server</string>
    <key>ProgramArguments</key>
    <array>
        <string>/Users/username/.lmstudio/bin/lms</string>
        <string>server</string>
        <string>start</string>
        <string>--port</string>
        <string>1234</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/tmp/lmstudio.log</string>
    <key>StandardErrorPath</key>
    <string>/tmp/lmstudio.error.log</string>
</dict>
</plist>
```

有効化：
```bash
launchctl load ~/Library/LaunchAgents/com.lmstudio.server.plist
```

## トラブルシューティング

### よくある問題と解決策

**Q: 「command not found: lms」エラー**
```bash
# PATHを再読み込み
source ~/.zshrc
# または手動でPATHに追加
export PATH="$HOME/.lmstudio/bin:$PATH"
```

**Q: モデルダウンロードが失敗する**
```bash
# GUI経由でダウンロード後、CLIで確認
lms ls
# 表示されたモデルキーを使用してロード
lms load <model_key>
```

**Q: JSON生成が不安定**
```python
# リトライロジック実装例
import time

def get_json_response(prompt, max_retries=3):
    for i in range(max_retries):
        try:
            response = client.chat.completions.create(...)
            return json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            if i == max_retries - 1:
                raise
            time.sleep(1)
```

**Q: メモリ不足エラー**
```bash
# より小さい量子化バージョンを使用
lms get "TinySwallow-1.5B-Instruct-GGUF"
# Q4_K_M (1.01GB) を選択
```

## まとめ

LM Studio + TinySwallowの組み合わせにより、以下を実現しました：

✅ **5分でセットアップ完了** - Homebrewとコマンド数個で環境構築
✅ **低スペックPCでも動作** - 8GBメモリのMacBook Airで快適動作
✅ **JSON構造化出力** - 42フィールドの複雑な構造も完璧に生成
✅ **OpenAI互換API** - 既存コードの流用が容易
✅ **完全無料** - APIコスト不要でPoC開発に最適

特に、1.5Bという小規模モデルでありながら、複雑なJSON Schema制約を完璧に満たす出力が得られたのは驚きでした。大量データ処理やプロンプトの反復改善が必要な場面で、ローカルLLMは強力な選択肢となります。

## 今後の展望

- より大規模なモデル（7B、13B）への移行
- MLXバックエンドによる更なる高速化
- ファインチューニングによる特定タスクへの最適化
- エッジデバイスでの活用

## 参考資料

- [LM Studio公式ドキュメント](https://lmstudio.ai/docs)
- [Sakana AI - TinySwallow](https://github.com/SakanaAI/evolutionary-model-merge)
- [GGUF形式について](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [OpenAI API仕様](https://platform.openai.com/docs/api-reference)

---

**補足**: 本記事の環境構築手順は、macOS Sonoma 14.5、Apple M2チップ、メモリ8GBのMacBook Airで検証しています。Intel Macや他のOSでは動作しない可能性があります。